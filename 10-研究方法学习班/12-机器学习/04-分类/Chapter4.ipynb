{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  4.4 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We now consider an alternative and less direct approach to estimating these probabilities.\n",
    "\n",
    "We model the distribution of the predictors X separately in each of the response classes (i.e. given Y ), and then use Bayes’ theorem to flip these around into estimates for Pr(Y = k|X = x). When these distributions are assumed to be normal, it turns out that the model is very similar in formto logistic regression.\n",
    "\n",
    "Why do we need another method, when we have logistic regression? There are several reasons:\n",
    "* When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable.\n",
    "* If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\n",
    "* As mentioned in Section 4.3.5, linear discriminant analysis is popular when we have more than two response classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.4.1 Using Bayes’ Theorem for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Suppose that we wish to classify an observation into one of K classes, where K ≥ 2. \n",
    "\n",
    "Let $ \\pi _{k}$represent the overall or prior probability that a randomly chosen observation comes from the kth class. This is the probability that a given observation is associated with the kth category of the response variable Y.  Let $f_{k}\\equiv Pr(X=x|Y=k)$ denote the density function of X for an observation that comes from the kth class. Then Bayes’theorem states that:\n",
    "$$\n",
    "Pr(X=x|Y=k)=\\frac{\\pi _{k}f_{k}(x)  }{ {\\textstyle \\sum_{l=1}^{K}\\pi _{l}f_{l}(x)  } }=p_{k}(X)    (4.10)\n",
    "$$\n",
    "This suggests that instead of directly computing $p_{k}(X)$ as in Section 4.3.1, we can simply plug in estimates of $\\pi _{k}$ and $f_{k}(X)$ into above equation. In general, estimating $\\pi _{k}$ is easy if we have a random sample of Y s from the population: we simply compute the fraction of the training observations that belong to the kth class. However, estimating $f_{k}(X)$ tends to be more challenging, unless we assume some simple forms for these densities.\n",
    "\n",
    "And we refer to $p_{k}(X)$ as the posterior probability that an observation X = x belongs to the kth class. That is, it is the probability that the observation belongs to the kth class, given the predictor value for that observation.\n",
    "\n",
    "We know from Chapter 2 that the Bayes classifier, which classifies an observation to the class for which $p_{k}(X)$ is largest, has the lowest possible error rate out of all classifiers. (This is of course only true if the terms in above equation are all correctly specified.) Therefore, if we can find a way to estimate $p_{k}(X)$, then we can develop a classifier that approximates the Bayes classifier. Such an approach is the topic of the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Linear Discriminant Analysis for p = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, assume that p = 1, that is, we have only one predictor.We would like to obtain an estimate for $f_{k}(x)$ that we can plug into (4.10) in order to estimate $p_{k}(x)$. We will then classify an observation to the class for which $p_{k}(x)$ is greatest.\n",
    "\n",
    "In order to estimate $f_{k}(x)$, we will first make some assumptions about its form:\n",
    "$$\n",
    "f_{x} (x)=\\frac{1}{\\sqrt{2\\pi}\\sigma _{k}}exp(-\\frac{1}{2\\sigma _{k}^{2} }(x-\\mu _{k} )^2 ) (4.11)\n",
    "$$\n",
    "For now, let us further assume that $\\sigma _{1}^{2}= \\sigma _{2}^{2}=…=\\sigma _{K}^{2}$: that is, there is a shared variance term across all K classes, which for simplicity we can denote by $\\sigma ^{2}$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "Pr(X=x|Y=k)=\\frac{\\pi _{k}\\frac{1}{\\sqrt{2\\pi}\\sigma }exp(-\\frac{1}{2\\sigma ^{2} }(x-\\mu _{k} )^2 )  }{ {\\textstyle \\sum_{l=1}^{K}\\pi _{l}\\frac{1}{\\sqrt{2\\pi}\\sigma }exp(-\\frac{1}{2\\sigma ^{2} }(x-\\mu _{l} )^2 ) } }(4.12)\n",
    "$$\n",
    "\n",
    "The Bayes classifier involves assigning an observation X = x to the class for which (4.12) is largest. Taking the log of (4.12) and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which\n",
    "$$\n",
    "\\delta _{k}(x)=x\\cdot \\frac{\\mu _{k}}{\\sigma^2}-  \\frac{\\mu _{k}^2}{2\\sigma^2}+log(\\pi_{k})(4.13)\n",
    "$$\n",
    "For instance, if K = 2 and $\\pi_{1}=\\pi_{2}$, then the Bayes classifier assigns an observation to class 1 if $2x(\\mu_{2}-\\mu_{2})>\\mu_{1}^2-\\mu_{2}^2$, and to class 2 otherwise. In this case, the Bayes decision boundary corresponds to the point where:\n",
    "\n",
    "$$\n",
    "x=\\frac{\\mu _{1}^2-\\mu_{2}^2}{2(\\mu_{1}-\\mu_{2})}=\\frac{\\mu_{1}+\\mu_{2}}{2}  (4.14)\n",
    "$$\n",
    "But in a real-life situation, we are not able to calculate the Bayes classifier.\n",
    "\n",
    "In practice, even if we are quite certain of our assumption that X is drawn from a Gaussian distribution within each class, we still have to estimate the many parameters. The linear discriminant analysis (LDA) method approximates the Bayes classifier by plugging estimate for $\\pi_{k}, \\mu_{k}, \\sigma^2$ and following estimates are used:\n",
    "$$\n",
    "\\hat{\\mu}_{k} =\\frac{1}{n_{k} }\\sum_{i:y_{i}=k}^{}x_{i}  \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 =\\frac{1}{n-K }\\sum_{k=1}^{K}\\sum_{i:y_{i}=k}^{}(x_{i}-\\hat{\\mu}_{k})^2  (4.15) \n",
    "$$\n",
    "\n",
    "where n is the total number of training observations, and $n_{k}$ is the number of training observations in the kth class. In the absence of any additional information, LDA estimates $\\pi_{k}$ using the proportion of the training observations that belong to the kth class. In other words,\n",
    "$$\n",
    "\\hat{\\pi}_{k}=n_{k}/n(4.16)\n",
    "$$\n",
    "and the delta is :\n",
    "$$\n",
    "\\hat{\\delta} _{k}(x)=x\\cdot \\frac{\\hat{\\mu} _{k}}{\\hat{\\sigma}^2}-  \\frac{\\hat{\\mu} _{k}^2}{2\\hat{\\sigma}^2}+log(\\hat{\\pi}_{k})(4.17)\n",
    "$$\n",
    "The word linear in the classifier’s name stems from the fact that the discriminant functions $\\hat{\\delta}_{k}(x)$ in (4.17) are linear functions of x (as discriminant function opposed to a more complex function of x).\n",
    "![title](fig/fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Linear Discriminant Analysis for p > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extend the LDA classifier to the case of multiple predictors. To do this, we will assume that X = (X1, X2,…, Xp) is drawn from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific\n",
    "multivariate Gaussian mean vector and a common covariance matrix.\n",
    "\n",
    "The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution, with some correlation between each pair of predictors. Two examples of multivariate Gaussian distributions with p = 2 are shown in following figure:\n",
    "![title](fig/fig2.png)\n",
    "\n",
    "To indicate that a p-dimensional random variable X has a multvariate Gaussian distribution, we write $X ∼ N(\\mu, \\sum)$. Formally, the multivariate Gaussian density is defined as:\n",
    "$$\n",
    "f(x)=\\frac{1}{(2\\pi)^{p/2}|\\sum|^{1/2}} exp(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1} (x-\\mu) )(4.18)\n",
    "$$\n",
    "Plugging the density function for the kth class, fk(X = x), into (4.10) and performing a little bit of algebra reveals that the Bayes classifier assigns an observation X = x to the class for which\n",
    "$$\n",
    "\\delta_{k}(x)=x^T\\Sigma^{-1}\\mu_{k}-\\frac{1}{2}\\mu_{k}^T\\Sigma^{-1}\\mu_{k}+log\\pi_{k} (4.19)\n",
    "$$\n",
    "is largest. This is the vector/matrix version of (4.13).\n",
    "\n",
    "An example is shown in the following figure:\n",
    "![title](fig/fig3.png)\n",
    "The three ellipses represent regions that contain 95% of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which $\\delta_{k}(x) = \\delta{l}(x)$; i.e.\n",
    "$$\n",
    "x^T\\Sigma^{-1}\\mu_{k}-\\frac{1}{2}\\mu_{k}^T\\Sigma^{-1}\\mu_{k}=x^T\\Sigma^{-1}\\mu_{l}-\\frac{1}{2}\\mu_{l}^T\\Sigma^{-1}\\mu_{l}(4.20)\n",
    "$$\n",
    "for $k\\neq l$. (The log πk term from (4.19) has disappeared because each of the three classes has the same number of training observations;\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform LDA on the Default data in order to predict whether or not an individual will default on the basis of credit card balance and\n",
    "student status. The LDA model fit to the 10, 000 training samples results in a training error rate of 2.75%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate is: 0.0275 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix,recall_score,accuracy_score\n",
    "\n",
    "file=open('Default.csv')\n",
    "Default=pd.read_csv(file)\n",
    "file.close()\n",
    "\n",
    "map_=pd.Series([0,1],index=['No','Yes'])\n",
    "Default['var_student']=Default['student'].map(map_)\n",
    "Default['var_default']=Default['default'].map(map_)\n",
    "\n",
    "X_train=Default[['balance','var_student']]\n",
    "y_train=Default['var_default']\n",
    "\n",
    "lda=LinearDiscriminantAnalysis().fit(X_train,y_train)\n",
    "score=lda.score(X_train,y_train)\n",
    "\n",
    "print('error rate is: %.4f '%(1-score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sounds like a low error rate, but two caveats must be noted:\n",
    "* First of all, training error rates will usually be lower than test error rates, which are the real quantity of interest. In other words, we might expect this classifier to perform worse if we use it to predict whether or not a new set of individuals will default. The reason is that we specifically adjust the parameters of our model to do well on the training data. The higher the ratio of parameters p to number of samples n, the more we expect this overfitting to play a role. For overfitting these data we don’t expect this to be a problem, since p = 4 and n = 10, 000.\n",
    "* Second, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3.33%. In other words, the trivial null classifier will achieve an error rate that null is only a bit higher than the LDA training set error rate.\n",
    "\n",
    "\n",
    "In practice, a binary classifier such as this one can make two types of errors: it can incorrectly assign an individual who defaults to the no default category, or it can incorrectly assign an individual who does not default to the default category. A confusion matrix, shown for the Default confusion matrix data in Table, is a convenient way to display this information："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_0</th>\n",
       "      <th>true_1</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pred_0</th>\n",
       "      <td>9644</td>\n",
       "      <td>252</td>\n",
       "      <td>9896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_1</th>\n",
       "      <td>23</td>\n",
       "      <td>81</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>9667</td>\n",
       "      <td>333</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        true_0  true_1  total\n",
       "pred_0    9644     252   9896\n",
       "pred_1      23      81    104\n",
       "total     9667     333  10000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusion_table(column='pred_default'):\n",
    "    res=pd.DataFrame({})\n",
    "    list_=[]\n",
    "    for i in [0,1]:\n",
    "        for j in [0,1]:    \n",
    "            list_.append(len(Default[(Default[column]==j) &\n",
    "                                 (Default['var_default']==i)]))\n",
    "        list_.append(list_[0]+list_[1])    \n",
    "        res['%s'%i]=list_\n",
    "        list_=[]\n",
    "    res.index=['pred_0','pred_1','total']\n",
    "    res.columns=['true_0','true_1']\n",
    "    res['total']=res.apply(lambda x:x[0]+x[1],axis=1)\n",
    "    return res\n",
    "\n",
    "y_pred=lda.predict(X_train)\n",
    "y_prob=lda.predict_proba(X_train)[:,1]\n",
    "Default['pred_default']=y_pred\n",
    "confusion_table(column='pred_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_0</th>\n",
       "      <th>true_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pred_0</th>\n",
       "      <td>9644</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_1</th>\n",
       "      <td>23</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        true_0  true_1\n",
       "pred_0    9644     252\n",
       "pred_1      23      81"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusion_df(classifier,X_test,y_test):\n",
    "    confusion=confusion_matrix(y_test,classifier.predict(X_test))\n",
    "    res=pd.DataFrame(confusion.T,index=['pred_0','pred_1'],columns=['true_0','true_1'])\n",
    "    return res\n",
    "\n",
    "confusion_df(lda,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class-specific performance is also important in medicine and biology, where the terms sensitivity and specificity characterize the performance of sensitivity specificity a classifier or screening test.Why does LDA do such a poor job of classifying the customers who default? In other words, why does it have such a low sensitivity?\n",
    "\n",
    "As we have seen, LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers (if the Gaussian model is correct).That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, irrespective of which class the errors come from.\n",
    "\n",
    "The Bayes classifier works by assigning an observation to the class for which the posterior probability pk(X) is greatest. In the two-class case, this amounts to assigning an observation to the default class if:\n",
    "\n",
    "$$\n",
    "Pr(default=Yes|X=x)>0.5(4.21)\n",
    "$$\n",
    "\n",
    "Thus, the Bayes classifier, and by extension LDA, uses a threshold of 50% for the posterior probability of default in order to assign an observation to the default class. However, if we are concerned about incorrectly predicting the default status for individuals who default, then we can consider lowering this threshold. For instance,\n",
    "\n",
    "$$\n",
    "Pr(default=Yes|X=x)>0.2(4.22)\n",
    "$$\n",
    "\n",
    "The error rates that result from taking this approach are shown in the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_0</th>\n",
       "      <th>true_1</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pred_0</th>\n",
       "      <td>9432</td>\n",
       "      <td>138</td>\n",
       "      <td>9570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_1</th>\n",
       "      <td>235</td>\n",
       "      <td>195</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>9667</td>\n",
       "      <td>333</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        true_0  true_1  total\n",
       "pred_0    9432     138   9570\n",
       "pred_1     235     195    430\n",
       "total     9667     333  10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lower_threshould=lda.predict_proba(X_train)[:,1]>0.2\n",
    "Default['pred2_default']=y_pred_lower_threshould\n",
    "confusion_table('pred2_default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following figure illustrates the trade-off that results from modifying the threshold value for the posterior probability of default.Using a threshold of 0.5, as in (4.21), minimizes the overall error rate, shown as a black solid line. But when a threshold of 0.5 is used, the error rate among the individuals who default is quite high (blue dashed line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresholds=[]\n",
    "errors=[]\n",
    "sensitivities=[]\n",
    "for i in np.linspace(0.00,0.60,61):\n",
    "    y_pred_lower_threshould=lda.predict_proba(X_train)[:,1]>i\n",
    "    sensitivity=recall_score(y_train,y_pred_lower_threshould)\n",
    "    error=1-accuracy_score(y_train,y_pred_lower_threshould)\n",
    "    errors.append(error)\n",
    "    thresholds.append(i)\n",
    "    sensitivities.append(sensitivity)\n",
    "data=pd.DataFrame({'thresholds':thresholds,'errors':errors,'sensitivities':sensitivities})\n",
    "data['err_sensitivities']=1-data['sensitivities']\n",
    "\n",
    "fig,ax=plt.subplots(1,1)\n",
    "sns.lineplot(x='thresholds',y='errors',data=data,ax=ax)\n",
    "sns.lineplot(x='thresholds',y='err_sensitivities',data=data,ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is a popular graphic for simultaneously displaying the ROC curve two types of errors for all possible thresholds. The following figure displays the ROC curve for the LDA classifier on the training data. The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC) curve (AUC). An ideal ROC curve will hug the top left corner, so the larger area under the (ROC) curve the AUC the better the classifier. For this data the AUC is 0.95, which is close to the maximum of one so would be considered very good. We expect a classifier that performs no better than chance to have an AUC of 0.5 (when evaluated on an independent test set not used in model training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr,tpr,thresholds=roc_curve(y_train,y_prob)\n",
    "data=pd.DataFrame({'fpr':fpr,'tpr':tpr}) \n",
    "\n",
    "fig,ax=plt.subplots(1,1)\n",
    "sns.lineplot(x='fpr',y='tpr',data=data,ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc is: 0.95 \n"
     ]
    }
   ],
   "source": [
    "auc=roc_auc_score(y_train,y_prob)\n",
    "print('auc is: %.2f '%auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen above, varying the classifier threshold changes its true positive and false positive rate. These are also called the sensitivity and one sensitivity minus the specificity of our classifier. Since there is an almost bewildering specificity array of terms used in this context, we now give a summary. Following table shows the possible results when applying a classifier (or diagnostic test) to a population.\n",
    "![title](fig/fig9.png)\n",
    "\n",
    "And the following table lists many of the popular performance measures that are used in this context. The denominators for the false positive and true positive rates are the actual population counts in each class. In contrast, the denominators for the positive predictive value and the negative predictive value are the total predicted counts for each class.\n",
    "![title](fig/fig10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have discussed, LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class specific mean vector and a covariance matrix that is common to all K classes. Quadratic discriminant analysis (QDA) provides an alternative approach.\n",
    "\n",
    "Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’theorem in order to perform prediction.\n",
    "\n",
    "However, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form $X \\sim N(\\mu_{k}, \\Sigma_{k})$, where $\\Sigma_{k}$ is a covariance matrix for the kth class. Under this assumption, the Bayes classifier assigns an observation $X = x$ to the class for which:\n",
    "\n",
    "$$\n",
    "\\delta_{k}(x)=x^T\\Sigma^{-1}\\mu_{k}-\\frac{1}{2}\\mu_{k}^T\\Sigma_{k}^{-1}\\mu_{k}+log\\pi_{k}(4.23)\n",
    "$$\n",
    "\n",
    "is largest. So the QDA classifier involves plugging estimates for $\\Sigma_{k}, \\mu_{k}$, and $\\pi_{k}$ into (4.23), and then assigning an observation X = x to the class for which this quantity is largest. Unlike in (4.19), the quantity x appears as a quadratic function in (4.23). This is where QDA gets its name.\n",
    "\n",
    "Why does it matter whether or not we assume that the K classes share a common covariance matrix? In other words, why would one prefer LDA to\n",
    "QDA, or vice-versa? The answer lies in the bias-variance trade-off. When there are p predictors, then estimating a covariance matrix requires estimating p(p+1)/2 parameters. QDA estimates a separate covariance matrix for each class, for a total of Kp(p+1)/2 parameters. With 50 predictors this is some multiple of 1,225, which is a lot of parameters. By instead assuming that the K classes share a common covariance matrix, the LDA model becomes linear in x, which means there are Kp linear coefficients to estimate. \n",
    "\n",
    "Consequently, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. This can potentially lead to improved prediction performance. But there is a trade-off: if LDA’s assumption that the K classes share a common covariance matrix is badly off, then LDA can suffer from high bias. \n",
    "\n",
    "Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable.\n",
    "\n",
    "The following figure illustrates the performances of LDA and QDA in two scenarios. In the left-hand panel, the two Gaussian classes have a common correlation of 0.7 between $X_{1}$ and $X_{2}$\n",
    "![title](fig/fig11.png)\n",
    "\n",
    "PS:The Bayes (purple dashed), LDA (black dotted), and QDA (green solid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 A Comparison of Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we have considered three different classification approaches: logistic regression, LDA, and QDA. In Chapter 2, we also discussed the K-nearest neighbors (KNN) method. We now consider the types of scenarios in which one approach might dominate the others.\n",
    "\n",
    "Though their motivations differ, the logistic regression and LDA methods are closely connected.\n",
    "\n",
    "Consider the two-class setting with $p = 1$ predictor, and let p_{1}(x) and $p_{2}(x) = 1 - p_{1}(x)$ be the probabilities that the observation X = x belongs to class 1 and class 2, respectively. In the LDA framework, we can see from (4.12) and (4.13) (and a bit of simple algebra) that the log odds is given by:\n",
    "\n",
    "$$\n",
    "log(\\frac{p_{1}(x)}{1-p_{1}(x)} )=c_{0}+c_{1}x(4.24)\n",
    "$$\n",
    "\n",
    "where $c_{0}$ and $c_{1}$ are functions of $\\mu_{1}, \\mu_{2}$, and $\\sigma^2$. From (4.4), we know that in logistic regression,\n",
    "\n",
    "$$\n",
    "log(\\frac{p_{1}}{1-p_{1}} )=\\beta_{0}+\\beta_{1}x(4.25)\n",
    "$$\n",
    "\n",
    "Both (4.24) and (4.25) are linear functions of x. Hence, both logistic rgression and LDA produce linear decision boundaries. The only difference between the two approaches lies in the fact that $\\beta_{0}$ and $\\beta_{1}$ are estimated using maximum likelihood, whereas $c_{0}$ and $c_{1}$ are computed using the estimated mean and variance from a normal distribution.\n",
    "\n",
    "Recall from Chapter 2 that KNN takes a completely different approach from the classifiers seen in this chapter.KNN is a completely non-parametric approach: no assumptions are made about the shape of the decision boundary. Therefore, we can expect this approach to dominate LDA and logistic regression when the decision boundary is highly non-linear. On the other hand, KNN does not tell us which predictors are important.\n",
    "\n",
    "Finally, QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. Since QDA\n",
    "assumes a quadratic decision boundary, it can accurately model a wider range of problems than can the linear methods. Though not as flexible\n",
    "as KNN, QDA can perform better in the presence of a limited number of training observations because it does make some assumptions about the form of the decision boundary.\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "To illustrate the performances of these four classification approaches, we generated data from six different scenarios.In three of the scenarios, the Bayes decision boundary is linear, and in the remaining scenarios it is non-linear. For each scenario, we produced 100 random training data sets. On each of these training sets, we fit each method to the data and computed the resulting test error rate on a large test set.\n",
    "![title](fig/fig12.png)\n",
    "![title](fig/fig13.png)\n",
    "\n",
    "* Scenario 1: There were 20 training observations in each of two classes. The observations within each class were uncorrelated random normal variables with a different mean in each class.\n",
    "* Scenario 2: Details are as in Scenario 1, except that within each class, the two predictors had a correlation of 0.5.\n",
    "* Scenario 3: We generated X1 and X2 from the t-distribution, with t-distribution 50 observations per class.\n",
    "* Scenario 4: The data were generated from a normal distribution, with a correlation of 0.5 between the predictors in the first class, and correlation of -0.5 between the predictors in the second class.\n",
    "* Scenario 5: Within each class, the observations were generated from a normal distribution with uncorrelated predictors. However, the responses were sampled from the logistic function using $X_{1}^2, X_{2}^2$, and $X_{1} × X_{2}$ as predictors.\n",
    "* Details are as in the previous scenario, but the responses were sampled from a more complicated non-linear function.\n",
    "\n",
    "These six examples illustrate that no one method will dominate the others in every situation.\n",
    "\n",
    "When the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well. When the boundaries are moderately non-linear, QDA may give better results. Finally, for much more complicated decision boundaries, a non-parametric approach such as KNN can be superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.3 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform LDA on the Smarket data. We fit the model using only the observations before 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probabilities of groups :\n",
      "['Down' 'Up']\n",
      "[0.49198397 0.50801603]\n",
      "Group means: (index=[Down,Up]),columns=[Lag1,Lag2]\n",
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n",
      "Coeffficients: \n",
      "[[-0.05544078 -0.0443452 ]]\n"
     ]
    }
   ],
   "source": [
    "file=open('Smarket.csv')\n",
    "Smarket=pd.read_csv(file)\n",
    "file.close()\n",
    "Smarket=Smarket.set_index('Year')\n",
    "\n",
    "data_train=Smarket[Smarket.index<2005]\n",
    "y_train=data_train['Direction']\n",
    "X_train=data_train[['Lag1','Lag2']]\n",
    "\n",
    "data_test=Smarket[Smarket.index==2005]\n",
    "y_test=data_test['Direction']\n",
    "X_test=data_test[['Lag1','Lag2']]\n",
    "\n",
    "lda=LinearDiscriminantAnalysis().fit(X_train,y_train)\n",
    "classes=lda.classes_\n",
    "priors=lda.priors_\n",
    "means=lda.means_\n",
    "coef=lda.coef_\n",
    "print('Prior probabilities of groups :')\n",
    "print(classes)\n",
    "print(priors)\n",
    "print('Group means: (index=[Down,Up]),columns=[Lag1,Lag2]')\n",
    "print(means)\n",
    "print('Coeffficients: ')\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observed in Section 4.5, the LDA and logistic regression predictions are almost identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        true_0  true_1\n",
      "pred_0      35      35\n",
      "pred_1      76     106\n",
      "Accuracy is: 0.5595238095238095\n"
     ]
    }
   ],
   "source": [
    "y_pred=lda.predict(X_test)\n",
    "print(confusion_df(lda,X_test,y_test))\n",
    "print('Accuracy is: {}'.format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a 50% threshold to the posterior probabilities allows us to recreate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "y_pred_threshould=(lda.predict_proba(X_test)>=0.5)[:,0]\n",
    "print(y_pred_threshould.sum())\n",
    "y_pred_threshould=(lda.predict_proba(X_test)<0.5)[:,0]\n",
    "print(y_pred_threshould.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to use a posterior probability threshold other than 50% in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day — say, if the posterior probability is at least 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "y_pred_threshould=(lda.predict_proba(X_test)>0.9)[:,0]\n",
    "print(y_pred_threshould.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.4 Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit a QDA model to the Smarket data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probabilities of groups :\n",
      "['Down' 'Up']\n",
      "[0.49198397 0.50801603]\n",
      "Group means: (index=[Down,Up]),columns=[Lag1,Lag2]\n",
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n"
     ]
    }
   ],
   "source": [
    "qda=QuadraticDiscriminantAnalysis().fit(X_train,y_train)\n",
    "classes=qda.classes_\n",
    "priors=qda.priors_\n",
    "means=qda.means_\n",
    "\n",
    "print('Prior probabilities of groups :')\n",
    "print(classes)\n",
    "print(priors)\n",
    "print('Group means: (index=[Down,Up]),columns=[Lag1,Lag2]')\n",
    "print(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        true_0  true_1\n",
      "pred_0      30      20\n",
      "pred_1      81     121\n",
      "Accuracy is: 0.5992063492063492\n"
     ]
    }
   ],
   "source": [
    "y_pred=qda.predict(X_test)\n",
    "print(confusion_df(qda,X_test,y_test))\n",
    "print('Accuracy is: {}'.format(accuracy_score(y_test,y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
